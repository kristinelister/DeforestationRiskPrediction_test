{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load necessary libraries\n",
    "import sys\n",
    "import os\n",
    "import ee\n",
    "import geemap\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from IPython.display import HTML, display\n",
    "from ipyleaflet import Map, basemaps\n",
    "import random\n",
    "import json\n",
    "import time\n",
    "import ast\n",
    "#Download wri_change_dection from https://pypi.org/project/rw-dynamicworld-cd/0.0.1/ \n",
    "#!pip install pip install rw-dynamicworld-cd==0.0.1\n",
    "from wri_change_detection import preprocessing as npv\n",
    "from wri_change_detection import gee_classifier as gclass\n",
    "from wri_change_detection import post_classification_filters as pcf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Initialize earth engine\n",
    "try:\n",
    "    ee.Initialize()\n",
    "except Exception as e:\n",
    "    ee.Authenticate()\n",
    "    ee.Initialize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define seed number to reproduce results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_seed = 30"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load Hansen forest change data and the DRC country boundary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CRS and Transform:  EPSG:4326 [0.00025, 0, -180, 0, -0.00025, 80]\n"
     ]
    }
   ],
   "source": [
    "# load Hansen forest change data\n",
    "forestChange = ee.Image(\"UMD/hansen/global_forest_change_2019_v1_7\")\n",
    "\n",
    "# save projection information\n",
    "projection_ee = forestChange.projection()\n",
    "projection = projection_ee.getInfo()\n",
    "crs = projection.get('crs')\n",
    "crsTransform = projection.get('transform')\n",
    "scale = projection_ee.nominalScale().getInfo()\n",
    "print('CRS and Transform: ',crs, crsTransform)\n",
    "\n",
    "# Load DRC feature\n",
    "countryBoundaries = ee.FeatureCollection('projects/resource-watch-gee/gadm36_0')\n",
    "DRCBoundary = countryBoundaries.filterMetadata('GID_0','equals','COD').first().geometry()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mask tree cover loss to areas with greater than 30% tree cover in 2000 and in primary forest areas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load primary forest data\n",
    "primaryForest = ee.Image('UMD/GLAD/PRIMARY_HUMID_TROPICAL_FORESTS/v1/2001')\n",
    "\n",
    "# select tree cover loss band and unmask with value 0 to represent no loss\n",
    "tclYear = forestChange.select(['lossyear']).unmask(0)\n",
    "\n",
    "# define mask of primary forest areas\n",
    "primaryForestValid = primaryForest.eq(1);\n",
    "# define mask for where Hansen data is valid\n",
    "forestChangeValid = forestChange.select('datamask').eq(1)\n",
    "# define mask of tree cover loss > 30%\n",
    "forestCoverValid = forestChange.select('treecover2000').gte(30)\n",
    "\n",
    "# create final mask where (primaryForestValid = 1) AND (forestChangeValid = 1) AND (forestCoverValid = 1)\n",
    "treeCoverLossValid = primaryForestValid.bitwiseAnd(forestCoverValid).bitwiseAnd(forestChangeValid)\n",
    "\n",
    "# update tree cover loss year to mask invalid areas\n",
    "tclYearMasked = tclYear.updateMask(treeCoverLossValid)\n",
    "# now tclYearMasked = {0 when tree cover loss did not occur, 1-19 for year when tree cover loss occured} only in valid areas\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot masked tree cover loss to validate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "07708ec2546f46a5b5fb9c3cd22f35d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map(center=[-1.776125, 23.710125], controls=(WidgetControl(options=['position'], widget=HBox(children=(ToggleBâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "center = [-1.776125,23.710125]\n",
    "zoom = 4\n",
    "tclPalette = ['#0051ff',\n",
    "            '#f7f4f9','#f7f4f9',\n",
    "            '#e7e1ef','#e7e1ef',\n",
    "            '#d4b9da','#d4b9da',\n",
    "            '#c994c7','#c994c7',\n",
    "            '#df65b0','#df65b0',\n",
    "            '#e7298a','#e7298a',\n",
    "            '#ce1256','#ce1256',\n",
    "            '#980043','#980043',\n",
    "            '#67001f','#67001f']\n",
    "tclViz = {'min': 0, 'max': 19, 'palette': tclPalette}\n",
    "Map1 = geemap.Map(center=center, zoom=zoom,basemap=basemaps.Esri.WorldImagery,add_google_map = False)\n",
    "Map1.addLayer(forestChangeValid.updateMask(forestChangeValid),\n",
    "              {'min': 0, 'max': 1, 'palette': ['#f9261b','#5a1bf9']},name='Hansen Valid')\n",
    "Map1.addLayer(forestCoverValid.updateMask(forestCoverValid),\n",
    "              {'min': 0, 'max': 1, 'palette': ['#f7f91b','#28ce4c']},name='Tree Cover > 30')\n",
    "Map1.addLayer(primaryForestValid.updateMask(primaryForestValid),\n",
    "              {'min': 0, 'max': 1, 'palette': ['#f9801b','#9d1bf9']},name='Primary Forest')\n",
    "Map1.addLayer(forestChange.select(['lossyear']),tclViz,name='Tree Cover Loss UnMasked')\n",
    "Map1.addLayer(tclYearMasked,tclViz,name='Tree Cover Loss Masked')\n",
    "\n",
    "display(Map1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create three binary layers:\n",
    "1. Tree cover loss that occurred from 2001 through 2011, this is referred to as early loss, which we'll use to calculate the distance to previous loss\n",
    "2. Tree cover loss that occurred from 2018 through 2019, this is loss we wil use in evaluating our model\n",
    "3. Tree cover loss that occurred from 2012 through 2017, this is our reference loss which we will be modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "704d0d9289484d3dab30ecfa41481f54",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map(center=[-1.776125, 23.710125], controls=(WidgetControl(options=['position'], widget=HBox(children=(ToggleBâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Define mask when tree cover occurred from 2001 through 2011\n",
    "tclEarlyLoss = tclYearMasked.expression(\n",
    "    '(year>0 && year<12)', {\n",
    "      'year': tclYearMasked.select('lossyear')\n",
    "})\n",
    "# Define mask when tree cover occurred after 2017\n",
    "tclLaterLoss = tclYearMasked.expression(\n",
    "    '(year>17)', {\n",
    "      'year': tclYearMasked.select('lossyear')\n",
    "})\n",
    "\n",
    "# Define binary variable for tree cover loss that occurred from 2012 through 2017,\n",
    "# loss that occurred after 2017 is marked as 0\n",
    "tclReferenceLoss = tclYearMasked.expression(\n",
    "    '(year>11 && year<18) ? 1 : 0', {\n",
    "      'year': tclYearMasked.select('lossyear')\n",
    "})\n",
    "\n",
    "# Define mask for tree cover loss that occurred from 2012 through 2017\n",
    "referenceTreeCoverLossValid = treeCoverLossValid.bitwiseAnd(tclEarlyLoss.eq(0))\n",
    "\n",
    "# Mask tree cover loss years to get binary 0 for no tree cover loss and 1 for \n",
    "tclReferenceLoss = tclReferenceLoss.updateMask(referenceTreeCoverLossValid).gt(0)\n",
    "tclReferenceLoss = tclReferenceLoss.rename('loss')\n",
    "\n",
    "# Map layers to double check!\n",
    "Map2 = geemap.Map(center=center, zoom=zoom,basemap=basemaps.Esri.WorldImagery,add_google_map = False)\n",
    "Map2.addLayer(tclYearMasked,tclViz,name='Tree Cover Loss Masked')\n",
    "Map2.addLayer(tclEarlyLoss,{'min': 0, 'max': 1, 'palette': ['#f9261b','#5a1bf9']},name='Early Loss')\n",
    "Map2.addLayer(tclLaterLoss,{'min': 0, 'max': 1, 'palette': ['#f9261b','#5a1bf9']},name='Later Loss')\n",
    "Map2.addLayer(tclReferenceLoss,{'min': 0, 'max': 1, 'palette': ['#f9261b','#5a1bf9']},name='Historical Loss')\n",
    "display(Map2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gather training, testing, and validation points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We'll remove training points that are within a certain distance to the validation and test points\n",
    "distanceToFilter = 5000\n",
    "numPoints = 10000\n",
    "\n",
    "#Sample point locations that we will split into training, validation, and test sets\n",
    "point_locations = npv.getStratifiedSampleBandPoints(tclReferenceLoss, region=DRCBoundary, \n",
    "                                                       numPoints=numPoints, bandName='loss',seed=num_seed,\n",
    "                                                       geometries=True, projection=projection_ee)\n",
    "\n",
    "#Assign random value between 0 and 1 to split into training, validation, and test sets\n",
    "point_locations = point_locations.randomColumn(columnName='TrainingSplit', seed=num_seed)\n",
    "#First split training set from the rest, taking 70% of the points for training\n",
    "#Roughly 70% training, 30% for validation + testing\n",
    "training_split = 0.7\n",
    "training_locations = point_locations.filter(ee.Filter.lt('TrainingSplit', training_split))\n",
    "validation_and_test = point_locations.filter(ee.Filter.gte('TrainingSplit', training_split))\n",
    "\n",
    "#Define distance filter to remove training points within a certain distance of test points and validation points\n",
    "distFilter = ee.Filter.withinDistance(distance=distanceToFilter, leftField='.geo', rightField= '.geo', maxError= 1)\n",
    "join = ee.Join.inverted()\n",
    "training_locations = join.apply(training_locations, validation_and_test, distFilter);\n",
    "\n",
    "#Assign another random value between 0 and 1 to validation_and_test to split to validation and test sets\n",
    "validation_and_test = validation_and_test.randomColumn(columnName='ValidationSplit', seed=num_seed)\n",
    "#Of the 30% saved for validation + testing, half goes to validation and half goes to test\n",
    "#Meaning original sample will be 70% training, 15% validation, 15% testing\n",
    "validation_split = 0.5 \n",
    "validation_locations = validation_and_test.filter(ee.Filter.lt('ValidationSplit', validation_split))\n",
    "test_locations = validation_and_test.filter(ee.Filter.gte('ValidationSplit', validation_split))\n",
    "\n",
    "#Apply distance filter to remove validation points within a certain distance of test points\n",
    "validation_locations = join.apply(validation_locations, test_locations, distFilter);\n",
    "\n",
    "#Export these locations to an Earth Engine asset\n",
    "location_description = '{}_locations'\n",
    "location_assetID = 'projects/wri-datalab/gfw-ai4earth/PredictingDeforestation/DRC/TrainingPoints/TrainingPoints_0409_{}_locations'\n",
    "#location_assetID = 'users/listerkristineanne/CongoDeforestation/TrainingPoints/TrainingPoints_{}_locations'\n",
    "\n",
    "# export_results_task = ee.batch.Export.table.toAsset(\n",
    "#     collection=training_locations, \n",
    "#     description = location_description.format('training'), \n",
    "#     assetId = location_assetID.format('training'))\n",
    "# export_results_task.start()\n",
    "    \n",
    "# export_results_task = ee.batch.Export.table.toAsset(\n",
    "#     collection=validation_locations, \n",
    "#     description = location_description.format('validation'), \n",
    "#     assetId = location_assetID.format('validation'))\n",
    "# export_results_task.start()\n",
    "\n",
    "# export_results_task = ee.batch.Export.table.toAsset(\n",
    "#     collection=test_locations, \n",
    "#     description = location_description.format('test'), \n",
    "#     assetId = location_assetID.format('test'))\n",
    "# export_results_task.start()\n",
    " \n",
    "# #Wait for last export to finish\n",
    "# while export_results_task.active():\n",
    "#     print('Polling for task (id: {}).'.format(export_results_task.id))\n",
    "#     time.sleep(30)\n",
    "# print('Done with export.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictorVariableFolder = 'projects/wri-datalab/gfw-ai4earth/PredictingDeforestation/DRC/PredictorVariables/HistoricalFinal'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define predictor variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load elevation\n",
    "elevation = ee.Image(\"CGIAR/SRTM90_V4\")\n",
    "# Calculate slope\n",
    "slope = ee.Terrain.slope(elevation)\n",
    "\n",
    "\n",
    "# Create cost map for measuring distance, for now we'll list crossing any pixel as the same weight\n",
    "# Generate a constant image with value 1, clip it to the bounds of the DRC\n",
    "cost = ee.Image.constant(1).clip(DRCBoundary.bounds())\n",
    "maxDistance=50000\n",
    "\n",
    "# Calculate distance to loss from 2001 to 2011\n",
    "# First unmask earlyLoss to get target pixels\n",
    "earlyLossUnmasked = tclEarlyLoss.unmask(0)\n",
    "distanceToEarlyLoss = cost.cumulativeCost(source= earlyLossUnmasked, maxDistance=maxDistance).rename('earlyLossDistance')\n",
    "distanceToEarlyLoss = distanceToEarlyLoss.unmask(maxDistance)\n",
    "\n",
    "# Burn roads feature collection to image\n",
    "# Load roads data\n",
    "roads = ee.FeatureCollection(os.path.join(predictorVariableFolder,'Roads'))\n",
    "roads = roads.map(lambda x: x.set({'constant':1}))\n",
    "roadsImage = roads.reduceToImage(['constant'], ee.Reducer.first())\n",
    "# Convert image to binary 0:1 if there is a road\n",
    "roadsImage = roadsImage.unmask(0)\n",
    "# Calculate distance to roads\n",
    "distanceToRoads = cost.cumulativeCost(source= roadsImage, maxDistance=maxDistance).rename('roadsDistance')\n",
    "distanceToRoads = distanceToRoads.unmask(maxDistance)\n",
    "\n",
    "# Load rural complex\n",
    "ruralComplex = ee.Image(os.path.join(predictorVariableFolder,'RuralComplex'))\n",
    "# Calculate distance to rural complex\n",
    "distanceToRuralComplex = cost.cumulativeCost(source= ruralComplex, maxDistance=maxDistance).rename('ruralComplexDistance')\n",
    "distanceToRuralComplex = distanceToRuralComplex.unmask(maxDistance)\n",
    "\n",
    "\n",
    "# Load protected areas\n",
    "protectedAreas = ee.FeatureCollection(os.path.join(predictorVariableFolder,'ProtectedAreas'))\n",
    "protectedAreas = protectedAreas.map(lambda x: x.set({'constant':1}))\n",
    "protectedAreasImage = protectedAreas.reduceToImage(['constant'], ee.Reducer.first()).unmask(0)\n",
    "\n",
    "huntingAreas =  ee.FeatureCollection(os.path.join(predictorVariableFolder,'HuntingAreas'))\n",
    "huntingAreas = huntingAreas.map(lambda x: x.set({'constant':1}))\n",
    "huntingAreasImage = huntingAreas.reduceToImage(['constant'], ee.Reducer.first()).unmask(0)\n",
    "\n",
    "concessions = ee.FeatureCollection('projects/wri-datalab/gfw-ai4earth/PredictingDeforestation/DRC/PredictorVariables/HistoricalFinal/Concessions')\n",
    "concessions = concessions.map(lambda x: x.set({'constant':1}))\n",
    "concessionsImage = concessions.reduceToImage(['constant'], ee.Reducer.first()).unmask(0)\n",
    "\n",
    "mining = ee.FeatureCollection('projects/wri-datalab/gfw-ai4earth/PredictingDeforestation/DRC/PredictorVariables/HistoricalFinal/ActiveMining')\n",
    "mining = mining.map(lambda x: x.set({'constant':1}))\n",
    "miningImage = mining.reduceToImage(['constant'], ee.Reducer.first()).unmask(0)\n",
    "\n",
    "artistinalLogging = ee.FeatureCollection('projects/wri-datalab/gfw-ai4earth/PredictingDeforestation/DRC/PredictorVariables/HistoricalFinal/ArtisinalLogging')\n",
    "artistinalLogging = artistinalLogging.map(lambda x: x.set({'constant':1}))\n",
    "artistinalLoggingImage = artistinalLogging.reduceToImage(['constant'], ee.Reducer.first())\n",
    "# Convert image to binary 0:1 if there is a road\n",
    "artistinalLoggingImage = artistinalLoggingImage.unmask(0)\n",
    "# Calculate distance to roads\n",
    "distanceToArtistinalLogging = cost.cumulativeCost(source= artistinalLoggingImage, maxDistance=maxDistance).rename('artistinalLoggingDistance')\n",
    "distanceToArtistinalLogging = distanceToArtistinalLogging.unmask(maxDistance)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "localities = ee.FeatureCollection('projects/wri-datalab/gfw-ai4earth/PredictingDeforestation/DRC/PredictorVariables/HistoricalFinal/Localities')\n",
    "# Burn hunting areas feature collection to image\n",
    "localities = localities.map(lambda x: x.set({'constant':1}))\n",
    "localitiesImage = localities.reduceToImage(['constant'], ee.Reducer.first())\n",
    "# Convert image to binary 0:1 if there is a road\n",
    "localitiesImage = localitiesImage.unmask(0)\n",
    "# Calculate distance to roads\n",
    "distanceToLocalities = cost.cumulativeCost(source= localitiesImage, maxDistance=maxDistance).rename('localitiesDistance')\n",
    "distanceToLocalities = distanceToLocalities.unmask(maxDistance)\n",
    "\n",
    "\n",
    "navigableRivers = ee.FeatureCollection('projects/wri-datalab/gfw-ai4earth/PredictingDeforestation/DRC/PredictorVariables/HistoricalFinal/NavigableRivers')\n",
    "# Burn hunting areas feature collection to image\n",
    "navigableRivers = navigableRivers.map(lambda x: x.set({'constant':1}))\n",
    "navigableRiversImage = navigableRivers.reduceToImage(['constant'], ee.Reducer.first())\n",
    "# Convert image to binary 0:1 if there is a road\n",
    "navigableRiversImage = navigableRiversImage.unmask(0)\n",
    "# Calculate distance to roads\n",
    "distanceToNavigableRivers = cost.cumulativeCost(source= navigableRiversImage, maxDistance=maxDistance).rename('navigableRiversDistance')\n",
    "distanceToNavigableRivers = distanceToNavigableRivers.unmask(maxDistance)\n",
    "\n",
    "rivers = ee.FeatureCollection('projects/wri-datalab/gfw-ai4earth/PredictingDeforestation/DRC/PredictorVariables/HistoricalFinal/Rivers')\n",
    "# Burn hunting areas feature collection to image\n",
    "rivers = rivers.map(lambda x: x.set({'constant':1}))\n",
    "riversImage = rivers.reduceToImage(['constant'], ee.Reducer.first())\n",
    "# Convert image to binary 0:1 if there is a road\n",
    "riversImage = riversImage.unmask(0)\n",
    "# Calculate distance to roads\n",
    "distanceToRivers = cost.cumulativeCost(source= riversImage, maxDistance=maxDistance).rename('riversDistance')\n",
    "distanceToRivers = distanceToRivers.unmask(maxDistance)\n",
    "\n",
    "\n",
    "# Define list of predictor variable images and convert to a single image\n",
    "predictor_variable_list = [distanceToEarlyLoss,distanceToRuralComplex,distanceToRoads,\n",
    "                           protectedAreasImage,huntingAreasImage,distanceToArtistinalLogging,\n",
    "                           distanceToLocalities,distanceToNavigableRivers,distanceToRivers,\n",
    "                           miningImage,\n",
    "                           concessionsImage,elevation,slope]\n",
    "predictor_variable_image = ee.ImageCollection(predictor_variable_list).toBands()\n",
    "\n",
    "# Rename bands\n",
    "predictor_variable_names = ['earlyLossDistance','ruralComplexDistance','roadsDistance',\n",
    "                            'protectedAreas','huntingAreas','artistinalLoggingDistance',\n",
    "                            'localitiesDistance','navigableRiversDistance','riversDistance',\n",
    "                            'mining',\n",
    "                            'concessions','elevation','slope']\n",
    "predictor_variable_image = predictor_variable_image.rename(predictor_variable_names)\n",
    "\n",
    "#print(predictor_variable_image.getInfo())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sample predictor variables at training, validation, and test points "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define properties to export to an Earth Engine asset\n",
    "points_description = '{}_points'\n",
    "points_assetID = 'projects/wri-datalab/gfw-ai4earth/PredictingDeforestation/DRC/TrainingPoints/TrainingPoints_0409_{}_points'\n",
    "\n",
    "# Load the point locations assets\n",
    "training_locations_asset = ee.FeatureCollection(location_assetID.format('training'))\n",
    "validation_locations_asset = ee.FeatureCollection(location_assetID.format('validation'))\n",
    "test_locations_asset = ee.FeatureCollection(location_assetID.format('test'))\n",
    "#print(training_locations_asset.getInfo())\n",
    "# Sample predictor variable at location\n",
    "training_points = predictor_variable_image.sampleRegions(training_locations_asset, \n",
    "                                                         projection=projection_ee, geometries=True,tileScale=16)\n",
    "validation_points = predictor_variable_image.sampleRegions(validation_locations_asset, \n",
    "                                                           projection=projection_ee, geometries=True,tileScale=16)\n",
    "test_points = predictor_variable_image.sampleRegions(test_locations_asset, \n",
    "                                                     projection=projection_ee, geometries=True,tileScale=16)\n",
    "\n",
    "# # # Export results\n",
    "# export_results_task = ee.batch.Export.table.toAsset(\n",
    "#     collection=training_points, \n",
    "#     description = points_description.format('training'), \n",
    "#     assetId = points_assetID.format('training'))\n",
    "# export_results_task.start()\n",
    "\n",
    "# export_results_task = ee.batch.Export.table.toAsset(\n",
    "#     collection=validation_points, \n",
    "#     description = points_description.format('validation'), \n",
    "#     assetId = points_assetID.format('validation'))\n",
    "# export_results_task.start()\n",
    "\n",
    "# export_results_task = ee.batch.Export.table.toAsset(\n",
    "#     collection=test_points, \n",
    "#     description = points_description.format('test'), \n",
    "#     assetId = points_assetID.format('test'))\n",
    "# export_results_task.start()\n",
    "\n",
    "# #Wait for last export to finish\n",
    "# while export_results_task.active():\n",
    "#     print('Polling for task (id: {}).'.format(export_results_task.id))\n",
    "#     time.sleep(30)\n",
    "# print('Done with export.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define model parameters to test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define dictionaries of parameters and models to test\n",
    "#You can find the inputs for the parameters under the ee.Classifiers section of GEE\n",
    "pointsAssetID = 'projects/wri-datalab/gfw-ai4earth/PredictingDeforestation/DRC/TrainingPoints/TrainingPoints_0409_{}_points'\n",
    "# # Export results\n",
    "trainingPoints = ee.FeatureCollection(pointsAssetID.format('training'))\n",
    "testPoints = ee.FeatureCollection(pointsAssetID.format('test'))\n",
    "validationPoints = ee.FeatureCollection(pointsAssetID.format('validation'))\n",
    "\n",
    "rf_parameters = {'seed':[num_seed], \n",
    "          'numberOfTrees': [50,100], \n",
    "          'variablesPerSplit': [4,6], \n",
    "          'minLeafPopulation': [4,10,50], \n",
    "          'bagFraction': [None,0.5,.3], \n",
    "          'maxNodes': [None, 20, 50]\n",
    "         }\n",
    "#buildGridSearchList converts the parameter dictionary into a list of classifiers that can be used in cross-validation\n",
    "rf_classifier_list = gclass.buildGridSearchList(rf_parameters,'smileRandomForest')\n",
    "\n",
    "svm_parameters = {'decisionProcedure':[None]}\n",
    "svm_classifier_list = gclass.buildGridSearchList(svm_parameters,'libsvm')\n",
    "\n",
    "#maxent_parameters = {'minIterations':[10,100],'maxIterations':[50,200]}\n",
    "#maxent_classifier_list = gclass.buildGridSearchList(maxent_parameters,'gmoMaxEnt')\n",
    "\n",
    "classifier_list = rf_classifier_list+svm_classifier_list#+maxent_classifier_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use cross-validation to test models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Sample Points 8714\n",
      "Average Fold Size 2904\n",
      "Polling for task (id: FETOD566INYBLI4IFOCEETIL).\n",
      "Done with export.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#Name y_column\n",
    "y_column = 'loss'\n",
    "\n",
    "#Define assetId and description format to export to GEE\n",
    "cv_results_assetId = 'users/listerkristineanne/CongoDeforestation/TrainingPoints/CV_Results_20210412'\n",
    "cv_results_description = 'cv_export_20210305'\n",
    "\n",
    "#Load training points\n",
    "training_points = ee.FeatureCollection(points_assetID.format('training'))\n",
    "\n",
    "#Perform cross validation, returns a feature collection\n",
    "cv_results = gclass.kFoldCrossValidation(inputtedFeatureCollection = training_points, \n",
    "                                     propertyToPredictAsString = y_column, \n",
    "                                     predictors = predictor_variable_names, \n",
    "                                     listOfClassifiers = classifier_list,\n",
    "                                     k=3,seed=num_seed)\n",
    "#Export results to GEE\n",
    "export_results_task = ee.batch.Export.table.toAsset(\n",
    "        collection=cv_results, \n",
    "        description = cv_results_description, \n",
    "        assetId = cv_results_assetId)\n",
    "export_results_task.start()\n",
    "\n",
    "#Wait for export to finish\n",
    "while export_results_task.active():\n",
    "    print('Polling for task (id: {}).'.format(export_results_task.id))\n",
    "    time.sleep(30)\n",
    "print('Done with export.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate accuracy of models on validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------\n",
      "Final Model Validation Set Confusion Matrix\n",
      "                Predicted_False  Predicted_True\n",
      "_                                              \n",
      "Observed_False             1118             194\n",
      "Observed_True               201             844\n",
      "Final Model Validation Set Accuracy 0.8324140857021638\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Create empty dataframe to save results of cross validation\n",
    "accuracy_and_keys = pd.DataFrame()\n",
    "\n",
    "#Load cross-validation results\n",
    "results = ee.FeatureCollection(cv_results_assetId)\n",
    "#Get the best result by the cross validation score\n",
    "best_result = results.sort('Validation Score', False).first()\n",
    "\n",
    "#Load params as a dictionary\n",
    "params = best_result.get('Params').getInfo()\n",
    "params = ast.literal_eval(params)\n",
    "\n",
    "#Get the calssifier name\n",
    "classifierName = best_result.get('Classifier Type').getInfo()\n",
    "\n",
    "#Load classifier with best params\n",
    "best_model = gclass.defineClassifier(params,classifierName)\n",
    "\n",
    "#Load training and validation points\n",
    "training_points = ee.FeatureCollection(points_assetID.format('training'))\n",
    "validation_points = ee.FeatureCollection(points_assetID.format('validation'))\n",
    "\n",
    "#Train a classifier with the best params on the training data\n",
    "best_model = best_model.train(training_points, classProperty=y_column, \n",
    "                              inputProperties=predictor_variable_names, subsamplingSeed=num_seed)\n",
    "\n",
    "#Predict over validation data\n",
    "validation_points_predicted = validation_points.classify(best_model)\n",
    "\n",
    "#Get confusion matrix and accuracy score\n",
    "confusion_matrix = validation_points_predicted.errorMatrix(y_column, 'classification');\n",
    "accuracy = confusion_matrix.accuracy().getInfo()\n",
    "\n",
    "\n",
    "#Get confusion matrix and accuracy score\n",
    "confusion_matrix = validation_points_predicted.errorMatrix(y_column, 'classification');\n",
    "accuracy = confusion_matrix.accuracy().getInfo()\n",
    "print('--------------------------------------------------------------')\n",
    "print('Final Model Validation Set Confusion Matrix')\n",
    "print(gclass.pretty_print_confusion_matrix_binary(confusion_matrix.getInfo()))\n",
    "print('Final Model Validation Set Accuracy',accuracy)\n",
    "print('\\n')\n",
    "\n",
    "\n",
    "# #Predict over test data\n",
    "# test_points = ee.FeatureCollection(points_assetID.format('test'))\n",
    "\n",
    "# test_points_predicted = test_points.classify(best_model)\n",
    "\n",
    "# #Get confusion matrix and accuracy score\n",
    "# confusion_matrix = test_points_predicted.errorMatrix(y_column, 'classification');\n",
    "# accuracy = confusion_matrix.accuracy().getInfo()\n",
    "# print('--------------------------------------------------------------')\n",
    "# print('Final Model Test Set Confusion Matrix')\n",
    "# print(gclass.pretty_print_confusion_matrix_binary(confusion_matrix.getInfo()))\n",
    "# print('Final Model Test Set Accuracy',accuracy)\n",
    "# print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kappa Score:  0.6602396676752266\n",
      "Consumer Accuracy:  [[0.8521341463414634, 0.8076555023923445]]\n",
      "Producer Accuracy:  [[0.8476118271417741], [0.8131021194605009]]\n"
     ]
    }
   ],
   "source": [
    "print('Kappa Score: ',confusion_matrix.kappa().getInfo())\n",
    "print('Consumer Accuracy: ',confusion_matrix.consumersAccuracy().getInfo())\n",
    "print('Producer Accuracy: ',confusion_matrix.producersAccuracy().getInfo())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['earlyLossDistance', 'ruralComplexDistance', 'roadsDistance', 'protectedAreas', 'huntingAreas', 'artistinalLoggingDistance', 'localitiesDistance', 'navigableRiversDistance', 'riversDistance', 'mining', 'concessions', 'elevation', 'slope']\n"
     ]
    }
   ],
   "source": [
    "print(best_model.schema().getInfo())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Left: Historic Loss\n",
      "Right: Predicted Loss\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1b70b0a9d8a64bc2bb8bb527e9cf9b52",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map(center=[-1.776125, 23.710125], controls=(WidgetControl(options=['position'], widget=HBox(children=(ToggleBâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Use best model and predictor variable image to predict loss\n",
    "predicted_loss = predictor_variable_image.updateMask(referenceTreeCoverLossValid).classify(best_model)\n",
    "\n",
    "# Map layers to double check!\n",
    "# Map3 = geemap.Map(center=center, zoom=zoom,basemap=basemaps.Esri.WorldImagery,add_google_map = False)\n",
    "# #Map3.addLayer(tclYearMasked,tclViz,name='Tree Cover Loss Masked')\n",
    "# Map3.addLayer(tclReferenceLoss,{'min': 0, 'max': 1, 'palette': ['#98BC47','#DA6E9A']},name='Historical Loss')\n",
    "# Map3.addLayer(predicted_loss,{'min': 0, 'max': 1, 'palette': ['#98BC47','#DA6E9A']},name='Predicted Loss')\n",
    "# #Map3.addLayer(tclReferenceLoss.eq(predicted_loss),{'min': 0, 'max': 1, 'palette': ['#98BC47','#DA6E9A']},name='Accurate Prediction')\n",
    "\n",
    "left_layer = geemap.ee_tile_layer(tclReferenceLoss,{'min': 0, 'max': 1, 'palette': ['#98BC47','#DA6E9A']},name='Historical Loss')\n",
    "right_layer = geemap.ee_tile_layer(predicted_loss,{'min': 0, 'max': 1, 'palette': ['#98BC47','#DA6E9A']},name='Predicted Loss')\n",
    "Map3 = geemap.Map(center=center, zoom=zoom,basemap=basemaps.Esri.WorldImagery,add_google_map = False)\n",
    "Map3.split_map(left_layer, right_layer)\n",
    "print('Left: Historic Loss')\n",
    "print('Right: Predicted Loss')\n",
    "display(Map3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "resultsAssetID = 'users/listerkristineanne/CongoDeforestation/PredictedLoss04012'\n",
    "export_results_task = ee.batch.Export.image.toAsset(\n",
    "    image=predicted_loss, \n",
    "    description='predictedLoss', \n",
    "    assetId=resultsAssetID, \n",
    "    region=DRCBoundary, \n",
    "    crs=crs, \n",
    "    crsTransform=crsTransform, \n",
    "    maxPixels=1e13)\n",
    "export_results_task.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Polling for task (id: W7OHCL6YQ23SAD2LYKP4ZLLC).\n",
      "Polling for task (id: W7OHCL6YQ23SAD2LYKP4ZLLC).\n",
      "Polling for task (id: W7OHCL6YQ23SAD2LYKP4ZLLC).\n",
      "Polling for task (id: W7OHCL6YQ23SAD2LYKP4ZLLC).\n",
      "Polling for task (id: W7OHCL6YQ23SAD2LYKP4ZLLC).\n",
      "Polling for task (id: W7OHCL6YQ23SAD2LYKP4ZLLC).\n",
      "Polling for task (id: W7OHCL6YQ23SAD2LYKP4ZLLC).\n",
      "Polling for task (id: W7OHCL6YQ23SAD2LYKP4ZLLC).\n",
      "Polling for task (id: W7OHCL6YQ23SAD2LYKP4ZLLC).\n"
     ]
    }
   ],
   "source": [
    "correctPrediction = predicted_loss.eq(tclReferenceLoss)\n",
    "\n",
    "accuracy = correctPrediction.reduceRegion(reducer=ee.Reducer.mean(), \n",
    "                        geometry=DRCBoundary, \n",
    "                        scale=30, \n",
    "                        bestEffort=True, \n",
    "                        maxPixels=1e13, tileScale=4)\n",
    "accuracy = ee.Feature(None,accuracy)\n",
    "\n",
    "#Define assetId and description format to export to GEE\n",
    "accuracy_results_assetId = 'users/listerkristineanne/CongoDeforestation/TrainingPoints/Accuracy_Results_20210412'\n",
    "accuracy_results_description = 'Accuracy_Results_20210412'\n",
    "\n",
    "#Export results to GEE\n",
    "export_results_task = ee.batch.Export.table.toAsset(\n",
    "        collection=accuracy, \n",
    "        description = accuracy_results_description, \n",
    "        assetId = accuracy_results_assetId)\n",
    "export_results_task.start()\n",
    "\n",
    "#Wait for export to finish\n",
    "while export_results_task.active():\n",
    "    print('Polling for task (id: {}).'.format(export_results_task.id))\n",
    "    time.sleep(30)\n",
    "print('Done with export.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
